{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import environ\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('AZURE_OPENAI_KEY')\n",
    "os.environ[\"OPENAI_API_ENDPOINT\"] = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = os.getenv('AZURE_OPENAI_DEPLOYMENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL & Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Notes\n",
    "- Expose schematic information about their input, output and config through their `.input_schema` property, the `.output_schema` property and the `.config_schema` method\n",
    "- LCEL is a declarative way to compose Runnables into chains - any chain constructed with LCEL will also always automatically have sync, async, batch, and streaming support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Primitives\n",
    "#### 1. RunnableSequence\n",
    "- Invokes a series of Runnables sequentially\n",
    "- output of the previous runnable will be input to the next runnable in the chain\n",
    "- can use pipe operator to construct or by passing a list of RunnableSequence\n",
    "\n",
    "#### 2. Runnable Parallel\n",
    "- Invokes runnables concurrently, providing the same input to each. Construct using dict literal within a sequence or by passing a dict to RunnableParallel\n",
    "\n",
    "### Additional Methods\n",
    "- can be used to modify behavior such as retry policy, lifecycle listeners, make them configurable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging\n",
    "- You can use `set_debug` from `from langchain_core.globals import set_debug`. set_debug(True)\n",
    "- You can pass existing or custom callbacks to any give chain too:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```from langchain_core.tracers import ConsoleCallbackHandler\n",
    "\n",
    "chain.invoke(\n",
    "    ...,\n",
    "    config={'callbacks': [ConsoleCallbackHandler()]}\n",
    ")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Data through\n",
    "- RunnablePassthrough allows to pass inputs unchanged or with the addition of extra keys. This typically is used in conjuction with RunnableParallel to assign data to a new key in the map.\n",
    "- RunnablePassthrough() called on it’s own, will simply take the input and pass it through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1},\n",
       " 'extra': {'num': 1, 'multiplybythree': 3},\n",
       " 'modified': 2}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    extra=RunnablePassthrough.assign(multiplybythree=lambda x: x[\"num\"] * 3),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `passed` key was called with RunnablePassthrough() and so it passed on {'num': 1}.\n",
    "\n",
    "- In the second line with the key `extra`, we used RunnablePastshrough.assign with a lambda that multiplies the passed integer value by 3. So, `extra` was set with {'num': 1, 'mult': 3} which is the original value with the `'multiplybythree'` key added.\n",
    "\n",
    "- Finally, we set a third key called `modified` in the map which uses a labmda to set a single value adding 1 to the num, which resulted in modified key with the value of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "# Load Azure OpenAI model\n",
    "model = AzureChatOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def multiple_length_function(data):\n",
    "    return len(data[\"text1\"]) * len(data[\"text2\"])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "chain1 = prompt | model\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"key\") | RunnableLambda(length_function),\n",
    "        \n",
    "        \"b\": {\"text1\": itemgetter(\"key\"), \"text2\": itemgetter(\"value\")}\n",
    "        | RunnableLambda(multiple_length_function),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "All inputs to these functions need to be a **SINGLE** argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single input and unpacks it into multiple argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='4 + 24 equals 28.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_67802d9a6d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-e3ca10a9-2544-4b28-af01-5518027528f3-0', usage_metadata={'input_tokens': 14, 'output_tokens': 8, 'total_tokens': 22, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"key\": \"haha\", \"value\": \"hahaha\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use `itemgetter` to extract the input arguments\n",
    "- `\"a\"` becomes `4` because `itemgetter` gets the value of `\"haha\"` and that is passed through pipe operator to the RunnableLamba which passes the value to the `length_function` which returns length of `\"hahaha\"` which is `6`\n",
    "- `\"b\"` is `24` because we pass the dictionary of `{\"text1\": \"haha\", \"text2\": \"hahaha\"}` to the RunnableLambda that passes it into the `multiple_length_function`\n",
    "- the dictionary of `{\"a\":4, \"b\": 24}` is then passed as output to the prompt usuing the pipe operator (`|`)\n",
    "- the output of the prompt again is passed to the model\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Routing based on Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two ways to perform routing:**\n",
    "1. Using a `RunnableBranch`.\n",
    "2. Writing custom factory function that takes the input of a previous step and returns a runnable. Importantly, this should return a runnable and NOT actually execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"AZURE_OPENAI_KEY\"] = os.getenv(\"AZURE_OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Given the user question below, classify it as either being about `Bodybuilding`, `Jungian Psychology`, or `Other`.\n",
    "\n",
    "        Do not respond with more than one word.\n",
    "\n",
    "        <question>\n",
    "        {question}\n",
    "        </question>\n",
    "\n",
    "        Classification:\"\"\"\n",
    "    )\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bodybuilding'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"How do I bench press properly?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jungian'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"How do I do active imagination?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_building_chain =  ( PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in bodybuilding. \\\n",
    "        Always answer questions starting with \"As Dr. Mike Israetel from Renaissance Periodization told me\". \\\n",
    "        Respond to the following question:\n",
    "        \n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    "    | model\n",
    ")\n",
    "jungian_chain = (PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in Jungian Psychology and Theory. \\\n",
    "    Always answer questions starting with \"As Dr. C.G. Jung would say\". \\\n",
    "    Respond to the following question:\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\") | model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"bodybuilding\" in x[\"topic\"].lower(), body_building_chain),\n",
    "    (lambda x: \"jungian psychology\" in x[\"topic\"].lower(), jungian_chain),\n",
    "    general_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Interpreting dreams can be a fascinating and insightful process, but it's important to remember that dream interpretation is highly subjective and can vary greatly from person to person. Here are some steps you can take to help interpret your dreams:\\n\\n1. **Keep a Dream Journal**: Write down your dreams as soon as you wake up. Include as many details as possible, such as people, places, emotions, and any symbols that stood out.\\n\\n2. **Identify Common Themes**: Look for recurring themes, symbols, or emotions in your dreams. These can provide clues about what your subconscious mind is trying to communicate.\\n\\n3. **Consider Your Current Life Situation**: Reflect on what is happening in your waking life. Dreams often reflect our thoughts, feelings, and concerns. Ask yourself if any part of your dream relates to your current experiences or emotions.\\n\\n4. **Explore Symbolism**: Many dreams contain symbols that can have personal or universal meanings. For example, water might represent emotions, while flying could symbolize freedom or escape. Consider what these symbols mean to you personally.\\n\\n5. **Use Dream Dictionaries with Caution**: While dream dictionaries can offer some insights, they are not definitive. The meaning of a symbol can vary greatly depending on the individual and their unique experiences.\\n\\n6. **Pay Attention to Emotions**: The emotions you feel in a dream can be just as important as the events or symbols. They can provide insight into your subconscious mind and how you are processing your waking life.\\n\\n7. **Look for Patterns**: Over time, you may notice patterns in your dreams that can help you understand recurring issues or themes in your life.\\n\\n8. **Seek Professional Help if Needed**: If you find that your dreams are causing you distress or you are struggling to interpret them, consider speaking with a therapist or counselor who specializes in dream analysis.\\n\\nRemember, there is no one-size-fits-all approach to dream interpretation. Trust your intuition and personal insights as you explore the meanings of your dreams.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 402, 'prompt_tokens': 24, 'total_tokens': 426, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_67802d9a6d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-4162cb71-32e5-49a3-96fd-a75be598cddc-0', usage_metadata={'input_tokens': 24, 'output_tokens': 402, 'total_tokens': 426, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I interpret my dreams?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='2 + 2 equals 4.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 24, 'total_tokens': 32, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_67802d9a6d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-f7863eee-b91e-43c7-b340-e18c33f4da8f-0', usage_metadata={'input_tokens': 24, 'output_tokens': 8, 'total_tokens': 32, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing with Custom Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"bodybuilding\" in info[\"topic\"].lower():\n",
    "        return body_building_chain\n",
    "    elif \"jungian psychology\" in info[\"topic\"].lower():\n",
    "        return jungian_chain\n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n",
    "    route\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As Dr. Mike Israetel from Renaissance Periodization told me, growing your traps involves a combination of heavy compound movements and targeted isolation exercises. He recommends incorporating exercises like deadlifts, shrugs, and upright rows into your routine. Deadlifts are particularly effective because they engage the traps as stabilizers, allowing you to lift heavy weights. Shrugs, on the other hand, directly target the upper traps and can be performed with dumbbells or a barbell. Upright rows also help in hitting the traps from a different angle. Dr. Israetel emphasizes the importance of progressive overload and ensuring you are consistently increasing the weight or reps over time to stimulate muscle growth. Additionally, maintaining proper form and a full range of motion during these exercises is crucial for maximizing trap development.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 55, 'total_tokens': 211, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_67802d9a6d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-9b0c8fd6-13aa-420a-8e00-ab2412830513-0', usage_metadata={'input_tokens': 55, 'output_tokens': 156, 'total_tokens': 211, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I grow my traps?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Alchemy and dreams are both rich in symbolism and transformation, making them closely related in various philosophical and psychological contexts. Alchemy, historically, was the medieval precursor to modern chemistry, focused on the transmutation of base metals into noble metals like gold. However, it also had a deeply spiritual and mystical dimension, aiming for the transformation of the self and the attainment of enlightenment or the \"Philosopher\\'s Stone.\"\\n\\nDreams, on the other hand, are a natural phenomenon where the mind processes experiences, emotions, and subconscious thoughts during sleep. In the realm of psychology, particularly in the work of Carl Jung, dreams are seen as a way for the unconscious mind to communicate with the conscious self, often using symbolic language.\\n\\nJungian psychology draws a strong parallel between alchemy and dreams. Jung believed that the alchemical process was a metaphor for personal transformation and individuation—the process of becoming one\\'s true self. In this view, the symbols and stages of alchemy (such as nigredo, albedo, citrinitas, and rubedo) correspond to stages of psychological development and transformation that can also be reflected in dreams.\\n\\nTherefore, alchemy and dreams are interconnected through their shared use of symbolism and their focus on transformation and self-discovery. Both can be seen as processes that guide individuals toward greater self-awareness and personal growth.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 268, 'prompt_tokens': 27, 'total_tokens': 295, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_67802d9a6d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-e14fe2fc-663b-43fc-ac27-9006a62408ee-0', usage_metadata={'input_tokens': 27, 'output_tokens': 268, 'total_tokens': 295, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"what does alchemy have to do with dreams?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binding runtime arguments\n",
    "- we can use `Runnable.bind()` to pass arguments as constants so that we can have access to them even within a runnable sequence where the argument is not part of the output of preceding runnables in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\QUESTIONS: When did the Cuban Missile Crisis take place?\n",
      "OPTIONS:\n",
      "A) 1959\n",
      "B) 1962\n",
      "C) 1965\n",
      "D) 1969\n",
      "SOLUTION: B) 1962\n",
      "\n",
      "\\QUESTIONS: Which two countries were directly involved in the Cuban Missile Crisis?\n",
      "OPTIONS:\n",
      "A) United States and China\n",
      "B) United States and Soviet Union\n",
      "C) United States and North Korea\n",
      "D) United States and Vietnam\n",
      "SOLUTION: B) United States and Soviet Union\n",
      "\n",
      "\\QUESTIONS: Who was the President of the United States during the Cuban Missile Crisis?\n",
      "OPTIONS:\n",
      "A) Dwight D. Eisenhower\n",
      "B) Lyndon B. Johnson\n",
      "C) John F. Kennedy\n",
      "D) Richard Nixon\n",
      "SOLUTION: C) John F. Kennedy\n",
      "\n",
      "\\QUESTIONS: What was the primary reason for the Cuban Missile Crisis?\n",
      "OPTIONS:\n",
      "A) The Soviet Union's installation of nuclear missiles in Cuba\n",
      "B) The United States' invasion of Cuba\n",
      "C) The Cuban government's nationalization of American businesses\n",
      "D) The Soviet Union's blockade of Berlin\n",
      "SOLUTION: A) The Soviet Union's installation of nuclear missiles in Cuba\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out 4 flashcard questions with their options based on the topic given below.. Use the format\\n\\QUESTIONS:...\\nOPTIONS:...\\nSOLUTION:...\\n\\n\",\n",
    "        ),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "runnable = (\n",
    "    {\"topic\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(runnable.invoke(\"the cuban missile crisis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are some flashcard questions on the topic of the Cuban Missile Crisis:\n",
      "\n",
      "1. What was the Cuban Missile Crisis?\n",
      "2. When did the Cuban Missile Crisis take place?\n",
      "3. Which two superpowers were involved in the Cuban Missile Crisis?\n",
      "4. What event triggered the Cuban Missile Crisis?\n",
      "5. Who was the President of the United States during the Cuban Missile Crisis?\n",
      "6. Who was the leader of the Soviet Union during the Cuban Missile Crisis?\n",
      "7. What was the main reason the Soviet Union placed missiles in Cuba?\n",
      "8. How did the United States discover the presence of Soviet missiles in Cuba?\n",
      "9. What was the U.S. response to the discovery of missiles in Cuba?\n",
      "10. What was the role of the United Nations during the Cuban Missile Crisis?\n",
      "11. How did the Cuban Missile Crisis end?\n",
      "12. What were the terms of the agreement that ended the Cuban Missile Crisis?\n",
      "13. What was the significance of the naval blockade during the Cuban Missile Crisis?\n",
      "14. How close did the world come to nuclear war during the Cuban Missile Crisis?\n",
      "15. What were the long-term effects of the Cuban Missile Crisis on U.S.-Soviet relations?\n",
      "16. How did the Cuban Missile Crisis affect Cuba's relationship with the Soviet Union?\n",
      "17. What role did diplomacy play in resolving the Cuban Missile Crisis?\n",
      "18. What was the \"hotline\" established after the Cuban Missile Crisis?\n",
      "19. How did the Cuban Missile Crisis influence future U.S. foreign policy?\n",
      "20. What lessons were learned from the Cuban Missile Crisis?\n",
      "\n",
      "These questions cover a range of aspects related to the Cuban Missile Crisis, including its causes, key figures, events, and consequences.\n"
     ]
    }
   ],
   "source": [
    "runnable = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model.bind(stop=\"SOLUTION\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(runnable.invoke(\"the cuban missile crisis\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach OpenAI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = {\n",
    "    \"name\": \"return_questions\",\n",
    "    \"description\": \"extracts questions from raw text\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"raw_text\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The raw text of flashcard questions\",\n",
    "            },\n",
    "            \"questions\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"array of questions\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"question string\"\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"equation\", \"solution\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here are some flashcard questions related to World War II:\\n\\n1. What were the main causes of World War II?\\n2. When did World War II begin and end?\\n3. Which event is commonly considered the starting point of World War II?\\n4. Who were the Axis Powers in World War II?\\n5. Who were the Allied Powers in World War II?\\n6. What was the significance of the Treaty of Versailles in the context of World War II?\\n7. What was the Blitzkrieg strategy, and which country used it?\\n8. What was the significance of the Battle of Britain?\\n9. What was Operation Barbarossa?\\n10. What was the significance of the attack on Pearl Harbor?\\n11. Who were the leaders of the major countries involved in World War II (e.g., USA, UK, USSR, Germany, Japan, Italy)?\\n12. What was the Holocaust?\\n13. What was the significance of D-Day?\\n14. What were the major conferences held by the Allies during the war (e.g., Yalta, Potsdam)?\\n15. What was the Manhattan Project?\\n16. What were the outcomes of the atomic bombings of Hiroshima and Nagasaki?\\n17. What was the significance of the Battle of Stalingrad?\\n18. What was the role of women during World War II?\\n19. How did World War II impact the global economy?\\n20. What were the Nuremberg Trials?\\n21. What was the significance of the United Nations' formation after World War II?\\n22. How did World War II lead to the Cold War?\\n23. What was the significance of the Lend-Lease Act?\\n24. What was the role of propaganda during World War II?\\n25. What were the major technological advancements during World War II?\\n26. How did World War II affect colonial empires?\\n27. What was the significance of the Battle of Midway?\\n28. What was the impact of World War II on civilian populations?\\n29. What were the main resistance movements during World War II?\\n30. How did World War II change the geopolitical landscape of Europe?\\n\\nThese questions cover a broad range of topics related to World War II and can be used for educational purposes.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 446, 'prompt_tokens': 31, 'total_tokens': 477, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_67802d9a6d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-673384b1-ed40-40e7-a56b-47ff5ca0f6ac-0', usage_metadata={'input_tokens': 31, 'output_tokens': 446, 'total_tokens': 477, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need gpt-4 to solve this one correctly\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out flashcard questions to the following topic then return the list of questions.\",\n",
    "        ),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "runnable = {\"topic\": RunnablePassthrough()} | prompt | model\n",
    "runnable.invoke(\"world war 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fallbacks\n",
    "- we can use fallbacks at the runnable level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatAnthropic, ChatOpenAI\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "\n",
    "import httpx\n",
    "from openai import RateLimitError\n",
    "\n",
    "request = httpx.Request(\"GET\", \"/\")\n",
    "response = httpx.Response(200, request=request)\n",
    "error = RateLimitError(\"rate limit\", response=response, body=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we set max_retries = 0 to avoid retrying on RateLimits, etc\n",
    "openai_llm = ChatOpenAI(max_retries=0)\n",
    "anthropic_llm = ChatAnthropic()\n",
    "llm = openai_llm.with_fallbacks([anthropic_llm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit error\n"
     ]
    }
   ],
   "source": [
    "# Let's use just the OpenAI LLm first, to show that we run into an error\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(openai_llm.invoke(\"Why did the chicken cross the road?\"))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=' I don\\'t have enough context to determine the chicken\\'s true motivation, but the classic punchline is: \"To get to the other side!\" It\\'s an anti-joke playing on the double meaning of \"the other side\" referring to either the other side of the road, or the afterlife. Without more details, I\\'d just be clucking in the dark trying to explain this chicken\\'s reasoning.'\n"
     ]
    }
   ],
   "source": [
    "# Now let's try with fallbacks to Anthropic\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(llm.invoke(\"Why did the chicken cross the road?\"))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\" I don't know, why did the kangaroo cross the road?\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a nice assistant who always includes a compliment in your response\",\n",
    "        ),\n",
    "        (\"human\", \"Why did the {animal} cross the road\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(chain.invoke({\"animal\": \"kangaroo\"}))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create a chain with a ChatModel\n",
    "# We add in a string output parser here so the outputs between the two are the same type\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a nice assistant who always includes a compliment in your response\",\n",
    "        ),\n",
    "        (\"human\", \"Why did the {animal} cross the road\"),\n",
    "    ]\n",
    ")\n",
    "# Here we're going to use a bad model name to easily create a chain that will error\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-fake\")\n",
    "bad_chain = chat_prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a chain with the normal OpenAI model\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Instructions: You should always include a compliment in your response.\n",
    "\n",
    "Question: Why did the {animal} cross the road?\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "llm = OpenAI()\n",
    "good_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nAnswer: The turtle crossed the road to get to the other side! That's a great question, by the way - you have a great sense of humor!\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now create a final chain which combines the two\n",
    "chain = bad_chain.with_fallbacks([good_chain])\n",
    "chain.invoke({\"animal\": \"turtle\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
