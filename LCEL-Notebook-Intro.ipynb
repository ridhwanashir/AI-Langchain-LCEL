{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from os import environ\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "os.environ[\"AZURE_OPENAI_KEY\"] = os.getenv('AZURE_OPENAI_KEY')\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = os.getenv('AZURE_OPENAI_DEPLOYMENT')\n",
    "os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"] = os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL and Pipe Operator\n",
    "\n",
    "## Pipe Operator\n",
    "\n",
    "The `|` symbol chains together the different components from the output from one component as input into the next component.\n",
    "\n",
    "## Runnable Interface\n",
    "\n",
    "1. `invoke`: stream-back chunks of the response\n",
    "2. `stream`: call the chain on an input\n",
    "3. `batch`: call the chain on a list of inputs\n",
    "\n",
    "Corresponding async methods:\n",
    "1. `ainvoke`: stream back chunks of the response async\n",
    "2. `astream`: call the chain on an input async\n",
    "3. `abatch`: call the chain on a list of inputs async\n",
    "\n",
    "## Input & Output Schemas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure OpenAI model\n",
    "model = AzureChatOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prompt and Chain\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'topic': {'title': 'Topic', 'type': 'string'}},\n",
       " 'required': ['topic'],\n",
       " 'title': 'PromptInput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'topic': {'title': 'Topic', 'type': 'string'}},\n",
       " 'required': ['topic'],\n",
       " 'title': 'PromptInput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'AIMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ai',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessage',\n",
       "   'type': 'object'},\n",
       "  'AIMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Message chunk from an AI.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'AIMessageChunk',\n",
       "     'default': 'AIMessageChunk',\n",
       "     'enum': ['AIMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None},\n",
       "    'tool_call_chunks': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCallChunk'},\n",
       "     'title': 'Tool Call Chunks',\n",
       "     'type': 'array'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatMessage': {'additionalProperties': True,\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'chat',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessage',\n",
       "   'type': 'object'},\n",
       "  'ChatMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Chat Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ChatMessageChunk',\n",
       "     'default': 'ChatMessageChunk',\n",
       "     'enum': ['ChatMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatPromptValueConcrete': {'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n",
       "   'properties': {'messages': {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
       "       {'$ref': '#/$defs/HumanMessage'},\n",
       "       {'$ref': '#/$defs/ChatMessage'},\n",
       "       {'$ref': '#/$defs/SystemMessage'},\n",
       "       {'$ref': '#/$defs/FunctionMessage'},\n",
       "       {'$ref': '#/$defs/ToolMessage'},\n",
       "       {'$ref': '#/$defs/AIMessageChunk'},\n",
       "       {'$ref': '#/$defs/HumanMessageChunk'},\n",
       "       {'$ref': '#/$defs/ChatMessageChunk'},\n",
       "       {'$ref': '#/$defs/SystemMessageChunk'},\n",
       "       {'$ref': '#/$defs/FunctionMessageChunk'},\n",
       "       {'$ref': '#/$defs/ToolMessageChunk'}]},\n",
       "     'title': 'Messages',\n",
       "     'type': 'array'},\n",
       "    'type': {'const': 'ChatPromptValueConcrete',\n",
       "     'default': 'ChatPromptValueConcrete',\n",
       "     'enum': ['ChatPromptValueConcrete'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['messages'],\n",
       "   'title': 'ChatPromptValueConcrete',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'function',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessage',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Function Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'FunctionMessageChunk',\n",
       "     'default': 'FunctionMessageChunk',\n",
       "     'enum': ['FunctionMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'HumanMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'human',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessage',\n",
       "   'type': 'object'},\n",
       "  'HumanMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Human Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'HumanMessageChunk',\n",
       "     'default': 'HumanMessageChunk',\n",
       "     'enum': ['HumanMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},\n",
       "    'cache_read': {'title': 'Cache Read', 'type': 'integer'}},\n",
       "   'title': 'InputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Error'},\n",
       "    'type': {'const': 'invalid_tool_call',\n",
       "     'enum': ['invalid_tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error'],\n",
       "   'title': 'InvalidToolCall',\n",
       "   'type': 'object'},\n",
       "  'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'reasoning': {'title': 'Reasoning', 'type': 'integer'}},\n",
       "   'title': 'OutputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'StringPromptValue': {'description': 'String prompt value.',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'type': {'const': 'StringPromptValue',\n",
       "     'default': 'StringPromptValue',\n",
       "     'enum': ['StringPromptValue'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['text'],\n",
       "   'title': 'StringPromptValue',\n",
       "   'type': 'object'},\n",
       "  'SystemMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'system',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessage',\n",
       "   'type': 'object'},\n",
       "  'SystemMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'System Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'SystemMessageChunk',\n",
       "     'default': 'SystemMessageChunk',\n",
       "     'enum': ['SystemMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolCall': {'description': 'Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"name\": \"foo\",\\n            \"args\": {\"a\": 1},\\n            \"id\": \"123\"\\n        }\\n\\n    This represents a request to call the tool named \"foo\" with arguments {\"a\": 1}\\n    and an identifier of \"123\".',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'type': {'const': 'tool_call',\n",
       "     'enum': ['tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id'],\n",
       "   'title': 'ToolCall',\n",
       "   'type': 'object'},\n",
       "  'ToolCallChunk': {'description': 'A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\"foo\", args=\\'{\"a\":\\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\\'1}\\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\"\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\"\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\\'foo\\', args=\\'{\"a\":1}\\', index=0)]',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "     'title': 'Index'},\n",
       "    'type': {'const': 'tool_call_chunk',\n",
       "     'enum': ['tool_call_chunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'index'],\n",
       "   'title': 'ToolCallChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\\'42\\', tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \"stdout\": \"From the graph we can see that the correlation between x and y is ...\",\\n            \"stderr\": None,\\n            \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\"stdout\"],\\n            artifact=tool_output,\\n            tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'tool',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessage',\n",
       "   'type': 'object'},\n",
       "  'ToolMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Tool Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ToolMessageChunk',\n",
       "     'default': 'ToolMessageChunk',\n",
       "     'enum': ['ToolMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"input_tokens\": 350,\\n            \"output_tokens\": 240,\\n            \"total_tokens\": 590,\\n            \"input_token_details\": {\\n                \"audio\": 10,\\n                \"cache_creation\": 200,\\n                \"cache_read\": 100,\\n            },\\n            \"output_token_details\": {\\n                \"audio\": 10,\\n                \"reasoning\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},\n",
       "    'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},\n",
       "    'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens'],\n",
       "   'title': 'UsageMetadata',\n",
       "   'type': 'object'}},\n",
       " 'anyOf': [{'type': 'string'},\n",
       "  {'$ref': '#/$defs/StringPromptValue'},\n",
       "  {'$ref': '#/$defs/ChatPromptValueConcrete'},\n",
       "  {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
       "     {'$ref': '#/$defs/HumanMessage'},\n",
       "     {'$ref': '#/$defs/ChatMessage'},\n",
       "     {'$ref': '#/$defs/SystemMessage'},\n",
       "     {'$ref': '#/$defs/FunctionMessage'},\n",
       "     {'$ref': '#/$defs/ToolMessage'},\n",
       "     {'$ref': '#/$defs/AIMessageChunk'},\n",
       "     {'$ref': '#/$defs/HumanMessageChunk'},\n",
       "     {'$ref': '#/$defs/ChatMessageChunk'},\n",
       "     {'$ref': '#/$defs/SystemMessageChunk'},\n",
       "     {'$ref': '#/$defs/FunctionMessageChunk'},\n",
       "     {'$ref': '#/$defs/ToolMessageChunk'}]},\n",
       "   'type': 'array'}],\n",
       " 'title': 'AzureChatOpenAIInput'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do programmers prefer dark mode?\\n\\nBecause light attracts bugs!'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"Programming\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why did the data analyst bring a ladder to work?\\n\\nBecause they heard the data was in the cloud!',\n",
       " \"Why did the data scientist break up with the statistician?\\n\\nBecause they couldn't find any common variance!\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\n",
    "    {\"topic\": \"Data Analyst\"},\n",
    "    {\"topic\": \"Data Science\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a bank-related joke for you:\n",
      "\n",
      "Why did the banker switch careers?\n",
      "\n",
      "He lost interest!"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for s in chain.stream({\"topic\": \"Bank\"}):\n",
    "\n",
    "    time.sleep(0.1)  # Sleep for 0.1 seconds\n",
    "\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Stop Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the neural network go to therapy?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 14, 'total_tokens': 23, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_67802d9a6d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-c239c8ed-f11c-4624-aee3-fdd72a27cbae-0', usage_metadata={'input_tokens': 14, 'output_tokens': 9, 'total_tokens': 23, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model.bind(stop=\"\\n\")\n",
    "chain.invoke({\"topic\": \"Machine Learning\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Azure OpenAI Embeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_KEY\"],\n",
    "    chunk_size=100\n",
    ")\n",
    "\n",
    "# Create a more diverse and informative vector store\n",
    "texts = [\n",
    "    \"The Eiffel Tower in Paris is 324 meters tall\",\n",
    "    \"Photosynthesis is the process by which plants convert light into energy\",\n",
    "    \"The human genome contains approximately 3 billion base pairs\",\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"The Great Barrier Reef is the world's largest coral reef system\",\n",
    "    \"Quantum computing leverages principles of quantum mechanics for computation\",\n",
    "    \"The Mona Lisa was painted by Leonardo da Vinci in the early 16th century\",\n",
    "    \"Climate change is causing global temperatures to rise at an unprecedented rate\"\n",
    "]\n",
    "\n",
    "vectorstore = FAISS.from_texts(texts, embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "# use previous model\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context does not contain any information about the kind of music Ridhwan likes.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What kind of music does Ridhwan like?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Mona Lisa is a painting by Leonardo da Vinci created in the early 16th century.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is Mona Lisa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_2 = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer like a {character_type}.\"\"\"\n",
    "\n",
    "prompt_2 = ChatPromptTemplate.from_template(template_2)\n",
    "\n",
    "chain_2 = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"character_type\": itemgetter(\"character_type\")\n",
    "    }\n",
    "    | prompt_2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, the Eiffel Tower be a grand structure in Paris standin' 324 meters tall!\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_2.invoke({\"question\": \"What is Eiffel Tower?\", \"character_type\": \"pirate\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
